# Product Requirements Document (PRD)

Project: pk-code – High-Performance Multi-Phase Coding Agent
Owner: Team pk-code • Date: 2025-08-01

## 1. Vision / Goal
Build an evidence-driven AI coding agent that achieves state-of-the-art real-world performance on SWE-bench-style tasks while remaining maintainable and extensible.

## 2. Background
Research on top-scoring agents (Warp, Refact.ai, MarsCode, mini-SWE-agent) shows that a single agent with a robust scaffold augmented by targeted sub-agents (debugger, planner) and strong guardrails delivers the highest verified-bug-fix rates.

## 3. Scope & Features
1. Multi-phase orchestrator prompt (Pareto → Plan → Execute).
2. Optional sub-agents:
   • pk-debugger – one-shot PDB session.
   • pk-planner – reflection / plan revision.
3. Repository embeddings index for accurate file retrieval.
4. Automatic retry & model fail-over chain.
5. Guardrail directives between phases and after key tool calls.
6. Multi-patch voting mode (configurable).
7. Detailed logging, rollback safety, and metrics.

## 4. Non-Functional Requirements
• Deterministic default temperature (≤0.2) for reproducibility.
• All secrets redacted; no persistent shell sessions.
• <2 min average latency per typical SWE-bench task.

## 5. Milestones & Subtasks
### 5.1 Orchestrator & Prompts
- [ ] Integrate `pk-multi-phase-orchestrator.md` into Taskmaster config.
- [ ] Validate phase transitions & guardrail injections.

### 5.2 Sub-Agents
- [ ] Draft `pk-debugger.md` prompt.
- [ ] Implement `run_pdb_test(test_path)` tool wrapper.
- [ ] Draft `pk-planner.md` prompt.

### 5.3 Repository Index
- [ ] Choose embedding model (text-embedding-3-large).
- [ ] Build nightly FAISS index job.
- [ ] Add `search_index(query, top_k)` tool.

### 5.4 Retry & Fail-Over
- [ ] Wrap tool calls with retry logic.
- [ ] Configure fallback LLM (GPT-4o).

### 5.5 Voting Mode
- [ ] Implement multi-patch generator in execution loop.
- [ ] Select first passing patch via tests.

### 5.6 Documentation & Examples
- [ ] Update README and docs/examples/orchestrator-run.md.
- [ ] Add CHANGELOG entries.

### 5.7 QA & Benchmarking
- [ ] Add integration tests for each new tool.
- [ ] Run SWE-bench subset to baseline.

## 6. Open Questions
• Which vector DB hosting option (local FAISS vs. cloud)?
• Preferred logging sink (stdout, file, or remote).

## 7. Success Metrics
• ≥70 % success on SWE-bench Verified subset.
• <2 min p95 latency.
• <1 % JSON/tool failure rate after retries.